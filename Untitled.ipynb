{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e56caee5-b6e9-49ae-8a6a-a19abc6ec69c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T07:19:32.634176Z",
     "iopub.status.busy": "2024-07-12T07:19:32.618658Z",
     "iopub.status.idle": "2024-07-12T07:19:59.253649Z",
     "shell.execute_reply": "2024-07-12T07:19:59.249723Z",
     "shell.execute_reply.started": "2024-07-12T07:19:32.633176Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai-whisper\n",
      "  Downloading openai-whisper-20231117.tar.gz (798 kB)\n",
      "     ---------------------------------------- 0.0/798.6 kB ? eta -:--:--\n",
      "     - -------------------------------------- 30.7/798.6 kB ? eta -:--:--\n",
      "     - ----------------------------------- 41.0/798.6 kB 388.9 kB/s eta 0:00:02\n",
      "     ----- ------------------------------ 112.6/798.6 kB 930.9 kB/s eta 0:00:01\n",
      "     -------- ----------------------------- 184.3/798.6 kB 1.0 MB/s eta 0:00:01\n",
      "     ------------- ------------------------ 286.7/798.6 kB 1.3 MB/s eta 0:00:01\n",
      "     --------------------- ---------------- 450.6/798.6 kB 1.7 MB/s eta 0:00:01\n",
      "     ---------------------------- --------- 604.2/798.6 kB 1.9 MB/s eta 0:00:01\n",
      "     -------------------------------- ----- 686.1/798.6 kB 1.9 MB/s eta 0:00:01\n",
      "     -------------------------------------  788.5/798.6 kB 1.9 MB/s eta 0:00:01\n",
      "     -------------------------------------- 798.6/798.6 kB 1.9 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: numba in c:\\users\\sivaranjani\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai-whisper) (0.59.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\sivaranjani\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai-whisper) (1.26.0)\n",
      "Requirement already satisfied: torch in c:\\users\\sivaranjani\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai-whisper) (2.1.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sivaranjani\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai-whisper) (4.66.1)\n",
      "Collecting more-itertools (from openai-whisper)\n",
      "  Downloading more_itertools-10.3.0-py3-none-any.whl.metadata (36 kB)\n",
      "Collecting tiktoken (from openai-whisper)\n",
      "  Downloading tiktoken-0.7.0-cp311-cp311-win_amd64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in c:\\users\\sivaranjani\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from numba->openai-whisper) (0.42.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\sivaranjani\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tiktoken->openai-whisper) (2024.5.15)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\sivaranjani\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tiktoken->openai-whisper) (2.31.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\sivaranjani\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->openai-whisper) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\sivaranjani\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->openai-whisper) (4.8.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\sivaranjani\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->openai-whisper) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\sivaranjani\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->openai-whisper) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sivaranjani\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->openai-whisper) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\sivaranjani\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->openai-whisper) (2023.10.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\sivaranjani\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm->openai-whisper) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sivaranjani\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sivaranjani\\appdata\\roaming\\python\\python311\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sivaranjani\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.0.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sivaranjani\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sivaranjani\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch->openai-whisper) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\sivaranjani\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch->openai-whisper) (1.3.0)\n",
      "Downloading more_itertools-10.3.0-py3-none-any.whl (59 kB)\n",
      "   ---------------------------------------- 0.0/59.2 kB ? eta -:--:--\n",
      "   --------------------------- ------------ 41.0/59.2 kB 1.9 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 41.0/59.2 kB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 59.2/59.2 kB 620.3 kB/s eta 0:00:00\n",
      "Downloading tiktoken-0.7.0-cp311-cp311-win_amd64.whl (799 kB)\n",
      "   ---------------------------------------- 0.0/799.0 kB ? eta -:--:--\n",
      "    --------------------------------------- 10.2/799.0 kB ? eta -:--:--\n",
      "   ---- ----------------------------------- 92.2/799.0 kB 1.7 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 204.8/799.0 kB 1.8 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 276.5/799.0 kB 1.5 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 276.5/799.0 kB 1.5 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 389.1/799.0 kB 1.6 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 471.0/799.0 kB 1.5 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 471.0/799.0 kB 1.5 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 471.0/799.0 kB 1.5 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 542.7/799.0 kB 1.3 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 655.4/799.0 kB 1.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 716.8/799.0 kB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 747.5/799.0 kB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  798.7/799.0 kB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 799.0/799.0 kB 1.2 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: openai-whisper\n",
      "  Building wheel for openai-whisper (pyproject.toml): started\n",
      "  Building wheel for openai-whisper (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=801372 sha256=db91cd403f3ca82c99e361ff1850c3589f57d496886ce4fa14cbe4e52863a449\n",
      "  Stored in directory: c:\\users\\sivaranjani\\appdata\\local\\pip\\cache\\wheels\\55\\5d\\42\\c296ab046d52caa0adc0e3f159e98f011b3994a022d6282105\n",
      "Successfully built openai-whisper\n",
      "Installing collected packages: more-itertools, tiktoken, openai-whisper\n",
      "Successfully installed more-itertools-10.3.0 openai-whisper-20231117 tiktoken-0.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U openai-whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8b75f88-1a65-4328-b0d3-7d19eae20b87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T06:35:22.029310Z",
     "iopub.status.busy": "2024-07-12T06:35:22.029310Z",
     "iopub.status.idle": "2024-07-12T06:35:22.285977Z",
     "shell.execute_reply": "2024-07-12T06:35:22.283946Z",
     "shell.execute_reply.started": "2024-07-12T06:35:22.029310Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to retrieve model",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m wake_word \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjarvis\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mGPT4All\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/Users/YOUR_USERNAME_HERE/Library/Application Support/nomic.ai/GPT4All/ggml-model-gpt4all-falcon-q4_0.bin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m r \u001b[38;5;241m=\u001b[39m sr\u001b[38;5;241m.\u001b[39mRecognizer()\n\u001b[0;32m      4\u001b[0m tiny_model \u001b[38;5;241m=\u001b[39m whisper\u001b[38;5;241m.\u001b[39mload_model(tiny\u001b[38;5;241m.\u001b[39mpt)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gpt4all\\gpt4all.py:97\u001b[0m, in \u001b[0;36mGPT4All.__init__\u001b[1;34m(self, model_name, model_path, model_type, allow_download, n_threads, device, verbose)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m pyllmodel\u001b[38;5;241m.\u001b[39mLLModel()\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# Retrieve model and download if allowed\u001b[39;00m\n\u001b[1;32m---> 97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig: ConfigType \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gpt4all\\gpt4all.py:189\u001b[0m, in \u001b[0;36mGPT4All.retrieve_model\u001b[1;34m(model_name, model_path, allow_download, verbose)\u001b[0m\n\u001b[0;32m    187\u001b[0m     config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m GPT4All\u001b[38;5;241m.\u001b[39mdownload_model(model_filename, model_path, verbose\u001b[38;5;241m=\u001b[39mverbose, url\u001b[38;5;241m=\u001b[39murl)\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 189\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to retrieve model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m config\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to retrieve model"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from os import system\n",
    "import speech_recognition as sr\n",
    "from gpt4all import GPT4All\n",
    "import sys\n",
    "import whisper\n",
    "import warnings\n",
    "import time\n",
    "import os\n",
    "wake_word = 'jarvis'\n",
    "model = GPT4All(\"/Users/YOUR_USERNAME_HERE/Library/Application Support/nomic.ai/GPT4All/ggml-model-gpt4all-falcon-q4_0.bin\", allow_download=False)\n",
    "r = sr.Recognizer()\n",
    "tiny_model = whisper.load_model(tiny.pt)\n",
    "base_model = whisper.load_model(base.pt)\n",
    "listening_for_wake_word = True\n",
    "source = sr.Microphone() \n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='whisper.transcribe', lineno=114)\n",
    "\n",
    "if sys.platform != 'darwin':\n",
    "    import pyttsx3\n",
    "    engine = pyttsx3.init() \n",
    "\n",
    "def speak(text):\n",
    "    if sys.platform == 'darwin':\n",
    "        ALLOWED_CHARS = set(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789.,?!-_$:+-/ \")\n",
    "        clean_text = ''.join(c for c in text if c in ALLOWED_CHARS)\n",
    "        system(f\"say '{clean_text}'\")\n",
    "    else:\n",
    "        engine.say(text)\n",
    "        engine.runAndWait()\n",
    "\n",
    "def listen_for_wake_word(audio):\n",
    "    global listening_for_wake_word\n",
    "    with open(\"wake_detect.wav\", \"wb\") as f:\n",
    "        f.write(audio.get_wav_data())\n",
    "    result = tiny_model.transcribe('wake_detect.wav')\n",
    "    text_input = result['text']\n",
    "    if wake_word in text_input.lower().strip():\n",
    "        print(\"Wake word detected. Please speak your prompt to GPT4All.\")\n",
    "        speak('Listening')\n",
    "        listening_for_wake_word = False\n",
    "\n",
    "def prompt_gpt(audio):\n",
    "    global listening_for_wake_word\n",
    "    try:\n",
    "        with open(\"prompt.wav\", \"wb\") as f:\n",
    "            f.write(audio.get_wav_data())\n",
    "        result = base_model.transcribe('prompt.wav')\n",
    "        prompt_text = result['text']\n",
    "        if len(prompt_text.strip()) == 0:\n",
    "            print(\"Empty prompt. Please speak again.\")\n",
    "            speak(\"Empty prompt. Please speak again.\")\n",
    "            listening_for_wake_word = True\n",
    "        else:\n",
    "            print('User: ' + prompt_text)\n",
    "            output = model.generate(prompt_text, max_tokens=200)\n",
    "            print('GPT4All: ', output)\n",
    "            speak(output)\n",
    "            print('\\nSay', wake_word, 'to wake me up. \\n')\n",
    "            listening_for_wake_word = True\n",
    "    except Exception as e:\n",
    "        print(\"Prompt error: \", e)\n",
    "\n",
    "def callback(recognizer, audio):\n",
    "    global listening_for_wake_word\n",
    "    if listening_for_wake_word:\n",
    "        listen_for_wake_word(audio)\n",
    "    else:\n",
    "        prompt_gpt(audio)\n",
    "\n",
    "def start_listening():\n",
    "    with source as s:\n",
    "        r.adjust_for_ambient_noise(s, duration=2)\n",
    "    print('\\nSay', wake_word, 'to wake me up. \\n')\n",
    "    r.listen_in_background(source, callback)\n",
    "    while True:\n",
    "        time.sleep(1) \n",
    "\n",
    "if _name_ == '_main_':\n",
    "    start_listening()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175a4f1a-0df9-48ed-8f00-bdb23ee4f316",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
